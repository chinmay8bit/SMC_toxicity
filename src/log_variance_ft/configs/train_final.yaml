hydra:
  searchpath:
    - file://./../../mdlm/configs

defaults:
  - config  # Refers to 'config.yaml' inside 'mdlm/configs'
  - _self_

eval:
  checkpoint_path: 'kuleshov-group/mdlm-owt'

backbone: 'hf_dit'

loader: 
  eval_batch_size: 32

model:
  length: 100

training:
  ema: 0 # this disables EMA used by MDLM

finetuning:
  num_timesteps: 100
  batch_size: 32
  lr: 0.00001
  num_epochs: 100
  batches_per_epoch: 5
  patience: 10
  sample_onpolicy: true
  num_samples_for_reward_estimate: 1
  reward_estimate_method: 'logmeanexp'
  kl_weight: 1.0
  kl_method: 'backward'
  alpha: 0.05
  lora:
    enabled: true
    target_modules: ["attn_qkv", "attn_out", "mlp.0", "mlp.2"]
    r: 8
    lora_alpha: 8
    lora_dropout: 0
    bias: "none"
